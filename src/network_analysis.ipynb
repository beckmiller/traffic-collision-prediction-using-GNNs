{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.cluster import DBSCAN\n",
    "import networkx as nx\n",
    "from scipy import stats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def analyze_weather_correlation(nodes_df, accidents_df):\n",
    "    \"\"\"\n",
    "    Analyze correlation between weather and accident frequency\n",
    "    \"\"\"\n",
    "    # Merge weather data with accident counts\n",
    "    merged_df = pd.merge(nodes_df, accidents_df, \n",
    "                        left_on='node_id', \n",
    "                        right_on=['node_1', 'node_2'], \n",
    "                        how='left')\n",
    "    \n",
    "    correlations = {\n",
    "        'temperature': stats.pearsonr(merged_df['tavg'], merged_df['acc_count'])[0],\n",
    "        'precipitation': stats.pearsonr(merged_df['prcp'], merged_df['acc_count'])[0]\n",
    "    }\n",
    "    \n",
    "    return correlations\n",
    "\n",
    "def analyze_road_types(edge_features, accidents_df):\n",
    "    \"\"\"\n",
    "    Analyze accident frequency by road type\n",
    "    \"\"\"\n",
    "    road_type_accidents = {}\n",
    "    for road_type in edge_features.keys():\n",
    "        if road_type.startswith(('highway_', 'residential_')):\n",
    "            # Convert sparse tensor to dense for analysis\n",
    "            road_matrix = edge_features[road_type].to_dense()\n",
    "            accident_count = accidents_df[accidents_df['node_1_idx'].isin(road_matrix.nonzero()[0])]['acc_count'].sum()\n",
    "            road_type_accidents[road_type] = accident_count\n",
    "    \n",
    "    return road_type_accidents\n",
    "\n",
    "def identify_high_risk_intersections(adj_matrix, accidents_df, threshold_percentile=90):\n",
    "    \"\"\"\n",
    "    Identify high-risk intersections using betweenness centrality and accident frequency\n",
    "    \"\"\"\n",
    "    # Convert to networkx graph for centrality calculation\n",
    "    G = nx.from_scipy_sparse_matrix(adj_matrix.to_sparse())\n",
    "    \n",
    "    # Calculate betweenness centrality\n",
    "    centrality = nx.betweenness_centrality(G)\n",
    "    \n",
    "    # Create risk score combining centrality and accident frequency\n",
    "    risk_scores = {}\n",
    "    for node in G.nodes():\n",
    "        accident_count = accidents_df[\n",
    "            (accidents_df['node_1_idx'] == node) | \n",
    "            (accidents_df['node_2_idx'] == node)\n",
    "        ]['acc_count'].sum()\n",
    "        \n",
    "        risk_scores[node] = centrality[node] * accident_count\n",
    "    \n",
    "    # Identify high-risk nodes\n",
    "    threshold = np.percentile(list(risk_scores.values()), threshold_percentile)\n",
    "    high_risk_nodes = {k: v for k, v in risk_scores.items() if v > threshold}\n",
    "    \n",
    "    return high_risk_nodes\n",
    "\n",
    "def detect_hotspots(nodes_df, accidents_df, eps=0.1, min_samples=5):\n",
    "    \"\"\"\n",
    "    Detect accident hotspots using DBSCAN\n",
    "    \"\"\"\n",
    "    # Prepare data for clustering\n",
    "    accident_locations = nodes_df.merge(\n",
    "        accidents_df, \n",
    "        left_on='node_id', \n",
    "        right_on=['node_1', 'node_2']\n",
    "    )[['lat', 'lon', 'acc_count']]\n",
    "    \n",
    "    # Weight locations by accident count\n",
    "    weighted_locations = np.repeat(\n",
    "        accident_locations[['lat', 'lon']].values,\n",
    "        accident_locations['acc_count'].astype(int),\n",
    "        axis=0\n",
    "    )\n",
    "    \n",
    "    # Perform DBSCAN clustering\n",
    "    clustering = DBSCAN(eps=eps, min_samples=min_samples).fit(weighted_locations)\n",
    "    \n",
    "    # Analyze clusters\n",
    "    cluster_stats = pd.DataFrame({\n",
    "        'cluster': clustering.labels_,\n",
    "        'lat': weighted_locations[:, 0],\n",
    "        'lon': weighted_locations[:, 1]\n",
    "    }).groupby('cluster').agg({\n",
    "        'lat': 'mean',\n",
    "        'lon': 'mean',\n",
    "        'cluster': 'size'\n",
    "    }).rename(columns={'cluster': 'accident_count'})\n",
    "    \n",
    "    return cluster_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/sq/1nt0kg2x37j377lq7nqlpz4c0000gn/T/ipykernel_18373/753881424.py:6: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  adj_matrix = torch.load(f\"{base_path}/{state}/adj_matrix.pt\")\n",
      "/var/folders/sq/1nt0kg2x37j377lq7nqlpz4c0000gn/T/ipykernel_18373/753881424.py:12: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  edge_features = torch.load(f\"{base_path}/{state}/Edges/edge_features.pt\")\n"
     ]
    }
   ],
   "source": [
    "def load_network_data(base_path, state, year, month):\n",
    "    \"\"\"\n",
    "    Load all required data files for network analysis\n",
    "    \"\"\"\n",
    "    # Load adjacency matrix\n",
    "    adj_matrix = torch.load(f\"{base_path}/{state}/adj_matrix.pt\")\n",
    "    \n",
    "    # Load node features with weather data\n",
    "    nodes_df = pd.read_csv(f\"{base_path}/{state}/Nodes/node_features_{year}_{month}.csv\")\n",
    "    \n",
    "    # Load edge features\n",
    "    edge_features = torch.load(f\"{base_path}/{state}/Edges/edge_features.pt\")\n",
    "    \n",
    "    # Load traffic data\n",
    "    # traffic_features = torch.load(f\"{base_path}/{state}/Edges/edge_features_traffic_{year}.pt\")\n",
    "    \n",
    "    # Load accident data\n",
    "    accidents_df = pd.read_csv(f\"{base_path}/{state}/Accidents_Nearest_Street_{state}_Monthly.csv\")\n",
    "    accidents_df = accidents_df[(accidents_df['year'] == year) & (accidents_df['month'] == month)]\n",
    "    \n",
    "    return adj_matrix, nodes_df, edge_features, accidents_df\n",
    "\n",
    "adj_matrix, nodes_df, edge_features, accidents_df = load_network_data('/Users/beck/Documents/GitHub/ML4RoadSafety', 'CA', 2021, 11)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### convert edge features to df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network statistics:\n",
      "Number of nodes: 1242784\n",
      "Number of edges: 3061644\n",
      "Number of features: 28\n",
      "\n",
      "Dense matrix would require approximately 161105.16 GB\n",
      "Sparse representation uses approximately 0.32 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Converting edges:   0%|          | 45/3061644 [06:37<7442:39:43,  8.75s/edges]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 114\u001b[0m\n\u001b[1;32m    112\u001b[0m print_memory_estimate(edge_features, nodes_df)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;66;03m# Then convert if manageable\u001b[39;00m\n\u001b[0;32m--> 114\u001b[0m edge_df \u001b[38;5;241m=\u001b[39m \u001b[43medge_features_to_dataframe\u001b[49m\u001b[43m(\u001b[49m\u001b[43medge_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnodes_df\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[6], line 58\u001b[0m, in \u001b[0;36medge_features_to_dataframe\u001b[0;34m(edge_features, nodes_df)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;66;03m# Add values for each feature\u001b[39;00m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m feat_name, feat_tensor \u001b[38;5;129;01min\u001b[39;00m edge_features\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m     56\u001b[0m     \u001b[38;5;66;03m# Get value directly from sparse tensor\u001b[39;00m\n\u001b[1;32m     57\u001b[0m     edge_dict[feat_name] \u001b[38;5;241m=\u001b[39m feat_tensor\u001b[38;5;241m.\u001b[39mcoalesce()\u001b[38;5;241m.\u001b[39mvalues()[\n\u001b[0;32m---> 58\u001b[0m         (\u001b[43mfeat_tensor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcoalesce\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mindices()[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m==\u001b[39m source_idx) \u001b[38;5;241m&\u001b[39m \n\u001b[1;32m     59\u001b[0m         (feat_tensor\u001b[38;5;241m.\u001b[39mcoalesce()\u001b[38;5;241m.\u001b[39mindices()[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m target_idx)\n\u001b[1;32m     60\u001b[0m     ]\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m     62\u001b[0m edge_data\u001b[38;5;241m.\u001b[39mappend(edge_dict)\n\u001b[1;32m     63\u001b[0m pbar\u001b[38;5;241m.\u001b[39mupdate(\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "def edge_features_to_dataframe(edge_features, nodes_df):\n",
    "    \"\"\"\n",
    "    Convert PyTorch sparse edge features into a pandas DataFrame.\n",
    "    Memory-efficient version that avoids creating dense matrices.\n",
    "    Includes progress bars for tracking conversion progress.\n",
    "    \n",
    "    Parameters:\n",
    "        edge_features (dict): Dictionary of sparse tensors for each feature\n",
    "        nodes_df (pd.DataFrame): DataFrame containing node IDs for mapping\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with all edge features\n",
    "    \"\"\"\n",
    "    # Initialize empty list to store edge data\n",
    "    edge_data = []\n",
    "    \n",
    "    # Get indices and values directly from sparse tensor\n",
    "    first_feature = list(edge_features.keys())[0]\n",
    "    indices = edge_features[first_feature]._indices()\n",
    "    \n",
    "    # Create mapping dictionary for node IDs\n",
    "    node_id_mapping = nodes_df['node_id'].to_dict()\n",
    "    \n",
    "    # Process edges in batches to manage memory\n",
    "    batch_size = 10000\n",
    "    num_edges = indices.shape[1]\n",
    "    \n",
    "    # Create progress bar for overall progress\n",
    "    pbar = tqdm(total=num_edges, desc=\"Converting edges\", unit=\"edges\")\n",
    "    \n",
    "    for batch_start in range(0, num_edges, batch_size):\n",
    "        batch_end = min(batch_start + batch_size, num_edges)\n",
    "        \n",
    "        # Process each edge in the batch\n",
    "        for i in range(batch_start, batch_end):\n",
    "            source_idx = indices[0][i].item()\n",
    "            target_idx = indices[1][i].item()\n",
    "            \n",
    "            # Get source and target node IDs from mapping\n",
    "            source_id = node_id_mapping[source_idx]\n",
    "            target_id = node_id_mapping[target_idx]\n",
    "            \n",
    "            # Create dictionary for this edge\n",
    "            edge_dict = {\n",
    "                'source_node': source_id,\n",
    "                'target_node': target_id,\n",
    "            }\n",
    "            \n",
    "            # Add values for each feature\n",
    "            for feat_name, feat_tensor in edge_features.items():\n",
    "                # Get value directly from sparse tensor\n",
    "                edge_dict[feat_name] = feat_tensor.coalesce().values()[\n",
    "                    (feat_tensor.coalesce().indices()[0] == source_idx) & \n",
    "                    (feat_tensor.coalesce().indices()[1] == target_idx)\n",
    "                ].item()\n",
    "            \n",
    "            edge_data.append(edge_dict)\n",
    "            pbar.update(1)\n",
    "        \n",
    "        # Optional: Convert batch to DataFrame and free memory\n",
    "        if len(edge_data) >= 1000000:  # Process in chunks of 1 million edges\n",
    "            temp_df = pd.DataFrame(edge_data)\n",
    "            if 'final_df' not in locals():\n",
    "                final_df = temp_df\n",
    "            else:\n",
    "                final_df = pd.concat([final_df, temp_df], ignore_index=True)\n",
    "            edge_data = []  # Free memory\n",
    "    \n",
    "    pbar.close()\n",
    "    \n",
    "    # Convert remaining data to DataFrame\n",
    "    if edge_data:\n",
    "        temp_df = pd.DataFrame(edge_data)\n",
    "        if 'final_df' not in locals():\n",
    "            final_df = temp_df\n",
    "        else:\n",
    "            final_df = pd.concat([final_df, temp_df], ignore_index=True)\n",
    "    \n",
    "    return final_df\n",
    "\n",
    "def print_memory_estimate(edge_features, nodes_df):\n",
    "    \"\"\"\n",
    "    Print estimated memory usage before conversion\n",
    "    \"\"\"\n",
    "    num_nodes = len(nodes_df)\n",
    "    num_features = len(edge_features)\n",
    "    \n",
    "    # Get actual number of edges\n",
    "    first_feature = list(edge_features.keys())[0]\n",
    "    num_edges = edge_features[first_feature]._nnz()\n",
    "    \n",
    "    print(f\"Network statistics:\")\n",
    "    print(f\"Number of nodes: {num_nodes}\")\n",
    "    print(f\"Number of edges: {num_edges}\")\n",
    "    print(f\"Number of features: {num_features}\")\n",
    "    \n",
    "    # Estimate memory for dense representation\n",
    "    dense_memory_gb = (num_nodes * num_nodes * num_features * 4) / (1024**3)\n",
    "    print(f\"\\nDense matrix would require approximately {dense_memory_gb:.2f} GB\")\n",
    "    \n",
    "    # Estimate memory for sparse representation\n",
    "    sparse_memory_gb = (num_edges * num_features * 4) / (1024**3)\n",
    "    print(f\"Sparse representation uses approximately {sparse_memory_gb:.2f} GB\")\n",
    "\n",
    "# Example usage:\n",
    "# First check memory requirements\n",
    "print_memory_estimate(edge_features, nodes_df)\n",
    "# Then convert if manageable\n",
    "edge_df = edge_features_to_dataframe(edge_features, nodes_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "len(right_on) must equal len(left_on)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m weather_correlation \u001b[38;5;241m=\u001b[39m \u001b[43manalyze_weather_correlation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnodes_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccidents_df\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(weather_correlation)\n",
      "Cell \u001b[0;32mIn[5], line 28\u001b[0m, in \u001b[0;36manalyze_weather_correlation\u001b[0;34m(nodes_df, accidents_df)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;124;03mAnalyze correlation between weather and accident frequency\u001b[39;00m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# Merge weather data with accident counts\u001b[39;00m\n\u001b[0;32m---> 28\u001b[0m merged_df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmerge\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnodes_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccidents_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mleft_on\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mnode_id\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mright_on\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mnode_1\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mnode_2\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mhow\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mleft\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     33\u001b[0m correlations \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     34\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtemperature\u001b[39m\u001b[38;5;124m'\u001b[39m: stats\u001b[38;5;241m.\u001b[39mpearsonr(merged_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtavg\u001b[39m\u001b[38;5;124m'\u001b[39m], merged_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124macc_count\u001b[39m\u001b[38;5;124m'\u001b[39m])[\u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m     35\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprecipitation\u001b[39m\u001b[38;5;124m'\u001b[39m: stats\u001b[38;5;241m.\u001b[39mpearsonr(merged_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprcp\u001b[39m\u001b[38;5;124m'\u001b[39m], merged_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124macc_count\u001b[39m\u001b[38;5;124m'\u001b[39m])[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     36\u001b[0m }\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m correlations\n",
      "File \u001b[0;32m/opt/miniconda3/envs/cs470/lib/python3.12/site-packages/pandas/core/reshape/merge.py:106\u001b[0m, in \u001b[0;36mmerge\u001b[0;34m(left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;129m@Substitution\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mleft : DataFrame or named Series\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     90\u001b[0m \u001b[38;5;129m@Appender\u001b[39m(_merge_doc, indents\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     91\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmerge\u001b[39m(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    104\u001b[0m     validate: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    105\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame:\n\u001b[0;32m--> 106\u001b[0m     op \u001b[38;5;241m=\u001b[39m \u001b[43m_MergeOperation\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    107\u001b[0m \u001b[43m        \u001b[49m\u001b[43mleft\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    108\u001b[0m \u001b[43m        \u001b[49m\u001b[43mright\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    109\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhow\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhow\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    110\u001b[0m \u001b[43m        \u001b[49m\u001b[43mon\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mon\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    111\u001b[0m \u001b[43m        \u001b[49m\u001b[43mleft_on\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mleft_on\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    112\u001b[0m \u001b[43m        \u001b[49m\u001b[43mright_on\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mright_on\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    113\u001b[0m \u001b[43m        \u001b[49m\u001b[43mleft_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mleft_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    114\u001b[0m \u001b[43m        \u001b[49m\u001b[43mright_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mright_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    115\u001b[0m \u001b[43m        \u001b[49m\u001b[43msort\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    116\u001b[0m \u001b[43m        \u001b[49m\u001b[43msuffixes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msuffixes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    117\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[43m        \u001b[49m\u001b[43mindicator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindicator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    119\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvalidate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    120\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m op\u001b[38;5;241m.\u001b[39mget_result()\n",
      "File \u001b[0;32m/opt/miniconda3/envs/cs470/lib/python3.12/site-packages/pandas/core/reshape/merge.py:681\u001b[0m, in \u001b[0;36m_MergeOperation.__init__\u001b[0;34m(self, left, right, how, on, left_on, right_on, axis, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[0m\n\u001b[1;32m    677\u001b[0m     \u001b[38;5;66;03m# stacklevel chosen to be correct when this is reached via pd.merge\u001b[39;00m\n\u001b[1;32m    678\u001b[0m     \u001b[38;5;66;03m# (and not DataFrame.join)\u001b[39;00m\n\u001b[1;32m    679\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(msg, \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m)\n\u001b[0;32m--> 681\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_specification\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    683\u001b[0m cross_col \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    684\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhow \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcross\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m/opt/miniconda3/envs/cs470/lib/python3.12/site-packages/pandas/core/reshape/merge.py:1402\u001b[0m, in \u001b[0;36m_MergeOperation._validate_specification\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1400\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mleft_on \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;01mNone\u001b[39;00m] \u001b[38;5;241m*\u001b[39m n\n\u001b[1;32m   1401\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhow \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcross\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mright_on) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mleft_on):\n\u001b[0;32m-> 1402\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlen(right_on) must equal len(left_on)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: len(right_on) must equal len(left_on)"
     ]
    }
   ],
   "source": [
    "weather_correlation = analyze_weather_correlation(nodes_df, accidents_df)\n",
    "print(weather_correlation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{}\n"
     ]
    }
   ],
   "source": [
    "def analyze_road_types(edge_features, accidents_df):\n",
    "    \"\"\"\n",
    "    Analyze accident frequency by road type\n",
    "    \"\"\"\n",
    "    road_type_accidents = {}\n",
    "    for road_type in edge_features.keys():\n",
    "        if road_type.startswith(('highway_', 'residential_')):\n",
    "            # Convert sparse tensor to dense for analysis\n",
    "            road_matrix = edge_features[road_type].to_dense()\n",
    "            accident_count = accidents_df[accidents_df['node_1_idx'].isin(road_matrix.nonzero()[0])]['acc_count'].sum()\n",
    "            road_type_accidents[road_type] = accident_count\n",
    "    \n",
    "    return road_type_accidents\n",
    "\n",
    "road_type_accidents = analyze_road_types(edge_features, accidents_df)\n",
    "print(road_type_accidents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'networkx' has no attribute 'from_scipy_sparse_matrix'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m high_risk_intersections \u001b[38;5;241m=\u001b[39m \u001b[43midentify_high_risk_intersections\u001b[49m\u001b[43m(\u001b[49m\u001b[43madj_matrix\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccidents_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mthreshold_percentile\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m90\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(high_risk_intersections)\n",
      "Cell \u001b[0;32mIn[5], line 59\u001b[0m, in \u001b[0;36midentify_high_risk_intersections\u001b[0;34m(adj_matrix, accidents_df, threshold_percentile)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;124;03mIdentify high-risk intersections using betweenness centrality and accident frequency\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;66;03m# Convert to networkx graph for centrality calculation\u001b[39;00m\n\u001b[0;32m---> 59\u001b[0m G \u001b[38;5;241m=\u001b[39m \u001b[43mnx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_scipy_sparse_matrix\u001b[49m(adj_matrix\u001b[38;5;241m.\u001b[39mto_sparse())\n\u001b[1;32m     61\u001b[0m \u001b[38;5;66;03m# Calculate betweenness centrality\u001b[39;00m\n\u001b[1;32m     62\u001b[0m centrality \u001b[38;5;241m=\u001b[39m nx\u001b[38;5;241m.\u001b[39mbetweenness_centrality(G)\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'networkx' has no attribute 'from_scipy_sparse_matrix'"
     ]
    }
   ],
   "source": [
    "high_risk_intersections = identify_high_risk_intersections(adj_matrix, accidents_df, threshold_percentile=90)\n",
    "print(high_risk_intersections)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs470",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
